# ğŸ§­ Topic Explorer  

## Overview  
**Topic Explorer** is a command-line tool that generates structured prompts from YAML templates and interacts with Large Language Models (LLMs) like OpenAI's GPT and Ollama-based models. It helps users create detailed prompts for AI interactions, retrieve model responses, and save them for further analysis.  

---

## ğŸš€ Features  
âœ… **Generate AI Prompts**: Convert structured YAML templates into well-formatted prompts  
âœ… **Interact with LLMs**: Communicate with OpenAI GPT models or Ollama-based models  
âœ… **Multi-Provider Support**: Works with multiple LLM providers like OpenAI and Ollama  
âœ… **Custom Configuration**: Define model parameters, temperature, and timeout settings  
âœ… **Save Responses**: Store generated prompts and responses for later use  
âœ… **Modular Design**: Easily extendable structure with separate `cmd`, `llm`, `paths`, and `config` modules  

---

## ğŸ“¦ Installation  

### Prerequisites  
Ensure you have the following installed:  
- **Go 1.24+**  
- **Homebrew (macOS users)** (for package management)  
- **Python (if using Pyenv for additional dependencies)**  

### Clone the Repository  
```sh  
git clone https://github.com/yourusername/topic-explorer.git  
cd topic-explorer  
```

### Install Dependencies  
```sh  
go mod tidy  
```

### Build the Project  
```sh  
go build -o topic-explorer  
```

---

## ğŸ¯ Usage  

### Generate a Prompt  
```sh  
./topic-explorer prompt --topic git --template resources/template.yaml --config resources/configs/git.yaml --output resources/output/git/prompt.txt  
```

### Interact with LLM  
```sh  
./topic-explorer llm --provider openai --model gpt-4 --prompt resources/output/git/prompt.txt --temperature 0.8  
```

### Generate & Retrieve a Response in One Step  
```sh  
./topic-explorer chat --topic git --provider openai --model gpt-4o  
```

---

## âš™ï¸ Configuration  

### Modify Default Configurations  
Default configurations are stored in `resources/default/config.yaml`. You can customize:  
```yaml  
provider: "ollama"  

model:  
  name: "phi4"  
  temperature: 0.8  
  streaming: true  

client:  
  timeout: "2m"  
  verbose_logging: true  
```

### Add New Topics  
To create a new topic, add a new YAML file inside `resources/configs/`:  
```yaml  
audience: "Developers"  
learning_stage: "Intermediate"  
topic: "Docker"  
context: "Software Engineering"  \analogies: "Shipping containers and package management"  
```

---

## ğŸ“‚ File Structure  
```plaintext  
topic-explorer  
â”œâ”€â”€ cmd/                # CLI Commands  
â”‚   â”œâ”€â”€ llm.go          # Handles LLM interactions  
â”‚   â”œâ”€â”€ chat.go         # Handles automated prompt generation + response  
â”‚   â”œâ”€â”€ prompt.go       # Handles prompt generation from YAML templates  
â”‚   â”œâ”€â”€ root.go         # CLI entry point  
â”‚   â””â”€â”€ vars.go         # Default values and constants  
â”œâ”€â”€ llm/                # LLM Handling  
â”‚   â”œâ”€â”€ provider.go     # Manages OpenAI & Ollama providers  
â”‚   â”œâ”€â”€ handler.go      # Handles streaming and request flow  
â”‚   â”œâ”€â”€ config.go       # Parses LLM configurations  
â”‚   â”œâ”€â”€ client.go       # LLM API client implementation  
â”œâ”€â”€ config/             # Configuration Parsing  
â”‚   â””â”€â”€ parser.go       # Parses YAML files  
â”œâ”€â”€ paths/              # File Path Management  
â”‚   â”œâ”€â”€ file.go         # File operations  
â”‚   â”œâ”€â”€ manager.go      # Path resolution for templates, outputs, and configs  
â”œâ”€â”€ resources/          # Prompt Templates & Output  
â”‚   â”œâ”€â”€ template.yaml   # Main prompt template  
â”‚   â”œâ”€â”€ configs/        # Topic-specific configurations  
â”‚   â”œâ”€â”€ output/         # Generated prompts & responses  
â”‚   â””â”€â”€ default/        # Default configurations  
â”œâ”€â”€ prompt/             # Prompt Generation Logic  
â”‚   â””â”€â”€ build.go        # Asynchronous processing for prompt building  
â”œâ”€â”€ main.go             # Entry point for execution  
â”œâ”€â”€ go.mod              # Go module dependencies  
â”œâ”€â”€ go.sum              # Dependency checksums  
```

---

## ğŸ”Œ Supported LLM Providers  

| Provider | Models Supported |  
|----------|-----------------|  
| **OpenAI** | gpt-4o, gpt-3.5-turbo |  
| **Ollama** | phi4, mistral, llama3 |  

### OpenAI Usage  
```sh  
./topic-explorer llm --provider openai --model gpt-4o  
```

### Ollama (Local Inference)  
```sh  
./topic-explorer llm --provider ollama --model phi4  
